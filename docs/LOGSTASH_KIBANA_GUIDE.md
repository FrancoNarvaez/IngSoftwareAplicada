# ğŸ“Š GuÃ­a Completa: Logstash Pipeline y VisualizaciÃ³n en Kibana

## ğŸ” **PARTE 1: Pipeline de Logstash Explicado**

### **UbicaciÃ³n del Pipeline**
```
logstash/pipeline/logstash.conf
```

### **AnatomÃ­a del Pipeline (3 Etapas)**

```conf
# ============================================
# ETAPA 1: INPUT - RecepciÃ³n de Logs
# ============================================
input {
  tcp {
    port => 5044              # Puerto donde escucha Logstash
    codec => json             # Espera logs en formato JSON
  }
}

# ============================================
# ETAPA 2: FILTER - Procesamiento (Opcional)
# ============================================
filter {
  # AquÃ­ se pueden agregar transformaciones:
  # - Parseo de campos
  # - Enriquecimiento con geolocalizaciÃ³n
  # - AgregaciÃ³n de metadatos
  # - Filtrado por condiciones
  
  # Ejemplo de filtro (actualmente vacÃ­o):
  # mutate {
  #   add_field => { "environment" => "development" }
  # }
}

# ============================================
# ETAPA 3: OUTPUT - Destino de Logs
# ============================================
output {
  # Output principal: Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "app-logs-%{+YYYY.MM.dd}"    # Ãndice diario (app-logs-2025-12-18)
  }
  
  # Output secundario: Consola (debugging)
  stdout { 
    codec => rubydebug                     # Formato legible en consola
  }
}
```

---

## ğŸ”§ **PARTE 2: CÃ³mo Funcionan los Filtros**

### **Tipos de Filtros Comunes**

#### 1. **Mutate** - Modificar campos
```conf
filter {
  mutate {
    add_field => { "service_name" => "store-service" }
    rename => { "msg" => "message" }
    remove_field => [ "unnecessary_field" ]
  }
}
```

#### 2. **Grok** - Parsear logs no estructurados
```conf
filter {
  grok {
    match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}" }
  }
}
```

#### 3. **Date** - Parsear timestamps
```conf
filter {
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }
}
```

#### 4. **Geoip** - Enriquecer con geolocalizaciÃ³n
```conf
filter {
  geoip {
    source => "client_ip"
    target => "geoip"
  }
}
```

---

## ğŸ“¤ **PARTE 3: CÃ³mo Llegan los Logs a Logstash**

### **Flujo Completo de un Log**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FLUJO DE LOGS EN ELK STACK                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Spring Boot genera log:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ LOG.info("Order created - ID: 123")â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    v
2. Logback convierte a JSON:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ {                                                           â”‚
   â”‚   "@timestamp": "2025-12-18T10:30:00.123Z",                â”‚
   â”‚   "level": "INFO",                                          â”‚
   â”‚   "logger_name": "com.jhipster.demo.store.ProductOrder",   â”‚
   â”‚   "message": "Order created - ID: 123",                    â”‚
   â”‚   "thread_name": "reactor-http-nio-2",                     â”‚
   â”‚   "level_value": 20000                                     â”‚
   â”‚ }                                                           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              v
3. Logback envÃ­a por TCP a Logstash:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ TCP Connection                     â”‚
   â”‚ localhost:5044                     â”‚
   â”‚ Payload: JSON (gzip comprimido)    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  v
4. Logstash recibe y procesa:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ INPUT: TCP port 5044               â”‚
   â”‚ FILTER: (opcional) parseo/enriquec.â”‚
   â”‚ OUTPUT: Elasticsearch + stdout     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  v
5. Elasticsearch indexa:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Ãndice: app-logs-2025-12-18        â”‚
   â”‚ Document ID: auto-generado UUID    â”‚
   â”‚ Campos indexados: todos            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  v
6. Kibana visualiza:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Query: GET /app-logs-*/_search     â”‚
   â”‚ Result: Logs en tiempo real        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” **PARTE 4: ConfiguraciÃ³n de Logback (Spring Boot)**

### **Archivo: `store/src/main/resources/logback-spring.xml`**

```xml
<!-- Appender de Logstash -->
<appender name="LOGSTASH" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
    <destination>localhost:5044</destination>
    
    <!-- Encoder JSON -->
    <encoder class="net.logstash.logback.encoder.LogstashEncoder">
        <customFields>{"service":"store-service"}</customFields>
    </encoder>
    
    <!-- ConfiguraciÃ³n de conexiÃ³n -->
    <keepAliveDuration>5 minutes</keepAliveDuration>
    <writeBufferSize>16384</writeBufferSize>
</appender>

<!-- Root logger -->
<root level="INFO">
    <appender-ref ref="LOGSTASH" />
    <appender-ref ref="CONSOLE" />
</root>
```

### **Campos JSON Generados AutomÃ¡ticamente**

| Campo | DescripciÃ³n | Ejemplo |
|-------|-------------|---------|
| `@timestamp` | Timestamp ISO8601 | `2025-12-18T10:30:00.123Z` |
| `level` | Nivel de log | `INFO`, `ERROR`, `DEBUG` |
| `logger_name` | Clase Java que generÃ³ el log | `com.jhipster.demo.store.web.rest.ProductOrderResource` |
| `message` | Mensaje del log | `Order created successfully - Order ID: 123` |
| `thread_name` | Thread de ejecuciÃ³n | `reactor-http-nio-2` |
| `stack_trace` | Stack trace (si es error) | `java.lang.NullPointerException: ...` |
| `service` | Nombre del servicio (custom) | `store-service` |

---

## ğŸ“Š **PARTE 5: VisualizaciÃ³n en Kibana**

### **Paso 1: Crear Index Pattern**

1. Abrir Kibana: http://localhost:5601
2. Ir a **Management** â†’ **Stack Management** â†’ **Index Patterns**
3. Click **Create index pattern**
4. Ingresar pattern: `app-logs-*` (el asterisco matchea todas las fechas)
5. Seleccionar **@timestamp** como Time field
6. Click **Create index pattern**

---

### **Paso 2: Ver Logs en Discover**

1. Ir a **Discover** (menÃº lateral)
2. Seleccionar index pattern: `app-logs-*`
3. Ajustar rango de tiempo: **Last 15 minutes** (esquina superior derecha)

---

### **Paso 3: Queries KQL (Kibana Query Language)**

#### **Ver logs de inicio de aplicaciÃ³n (startup)**
```kql
message: "Application startup completed successfully"
```

#### **Ver logs de creaciÃ³n de Ã³rdenes**
```kql
message: "Order created successfully"
```

#### **Ver logs de errores**
```kql
level: "ERROR"
```

#### **Ver logs de un servicio especÃ­fico**
```kql
logger_name: "com.jhipster.demo.store.web.rest.ProductOrderResource"
```

#### **Ver logs de operaciones de base de datos**
```kql
message: "Database Configuration" OR message: "R2DBC"
```

#### **Ver logs de health checks**
```kql
message: "health" OR message: "readiness"
```

#### **Combinaciones (AND/OR)**
```kql
level: "INFO" AND message: "Order created"
```

```kql
level: "ERROR" OR level: "WARN"
```

#### **Filtrar por rango de tiempo especÃ­fico**
```kql
@timestamp >= "2025-12-18T10:00:00" AND @timestamp <= "2025-12-18T11:00:00"
```

---

### **Paso 4: Crear Visualizaciones**

#### **1. GrÃ¡fico de LÃ­nea: Logs por Minuto**

1. Ir a **Visualize** â†’ **Create visualization**
2. Seleccionar **Line**
3. Configurar:
   - **Metrics**: Count
   - **Buckets**: X-Axis â†’ Date Histogram â†’ @timestamp â†’ Interval: **1 minute**
4. Click **Update** â†’ **Save**

#### **2. Pie Chart: Logs por Nivel (INFO, ERROR, WARN)**

1. **Visualize** â†’ **Create** â†’ **Pie**
2. Configurar:
   - **Metrics**: Count
   - **Buckets**: Split slices â†’ Terms â†’ Field: **level.keyword**
3. **Update** â†’ **Save**

#### **3. Data Table: Top 10 Loggers mÃ¡s activos**

1. **Visualize** â†’ **Create** â†’ **Data table**
2. Configurar:
   - **Metrics**: Count
   - **Buckets**: Split rows â†’ Terms â†’ Field: **logger_name.keyword** â†’ Size: 10
3. **Update** â†’ **Save**

#### **4. Tag Cloud: Palabras mÃ¡s frecuentes en mensajes**

1. **Visualize** â†’ **Create** â†’ **Tag cloud**
2. Configurar:
   - **Tags**: Terms â†’ Field: **message.keyword** â†’ Size: 20
3. **Update** â†’ **Save**

---

### **Paso 5: Crear Dashboard**

1. Ir a **Dashboard** â†’ **Create dashboard**
2. Click **Add** â†’ Seleccionar las visualizaciones creadas
3. Arrastrar y organizar paneles
4. Agregar filtros globales (ej: `level: "ERROR"`)
5. **Save dashboard** con nombre: "TechStore Monitoring"

---

## ğŸ”¥ **PARTE 6: Logs Enriquecidos que Agregamos**

### **Logs de Startup (StoreApp.java)**

#### **Antes:**
```
Application 'store' is running! Access URLs:
  Local: http://localhost:8080/
```

#### **DespuÃ©s:**
```
Application 'store' is running! Access URLs:
  Local: http://localhost:8080/
System Configuration - Java Version: 21.0.1, Max Memory: 4096MB, Total Memory: 512MB, Processors: 8
Database Configuration - R2DBC URL: r2dbc:mysql://****@localhost:3307/store (Reactive Driver)
Observability - Logstash logging enabled at localhost:5044
Application startup completed successfully - Ready to accept requests
```

**Query en Kibana:**
```kql
message: "Application startup completed successfully"
```

---

### **Logs de CreaciÃ³n de Ã“rdenes (ProductOrderResource.java)**

#### **Antes:**
```
REST request to save ProductOrder : ProductOrder{...}
```

#### **DespuÃ©s:**
```
Creating new order - Customer: 1, Items count: 3, Total amount: 2549.99, Payment method: CREDIT_CARD
Order created successfully - Order ID: 123, Customer: 1, Status: PENDING, Total: 2549.99
```

**Query en Kibana:**
```kql
message: "Order created successfully"
```

**Filtrar Ã³rdenes de mÃ¡s de $1000:**
```kql
message: "Order created successfully" AND message: /Total: [1-9][0-9]{3}/
```

---

### **Logs de Errores en Ã“rdenes**

#### **Nuevo Log:**
```
Order creation failed - Customer: 1, Error: Insufficient stock for product ID: 5
```

**Query en Kibana:**
```kql
message: "Order creation failed"
```

---

### **Logs de Operaciones de Listado**

#### **Nuevo Log:**
```
Fetching orders - Page: 0, Size: 20, Eager load: true
Orders retrieved successfully - Total orders: 156, Page items: 20
```

**Query en Kibana:**
```kql
message: "Orders retrieved successfully"
```

---

### **Logs de EliminaciÃ³n**

#### **Nuevo Log:**
```
Deleting order - Order ID: 123
Order deleted successfully - Order ID: 123
```

**Query en Kibana:**
```kql
message: "Order deleted successfully"
```

---

## ğŸ¯ **PARTE 7: Queries Avanzadas para Defensa**

### **1. Ver todas las operaciones de un customer especÃ­fico**
```kql
message: "Customer: 1"
```

### **2. Ver logs de los Ãºltimos 5 minutos con errores**
```kql
level: "ERROR" AND @timestamp >= now-5m
```

### **3. Ver cuÃ¡ntas Ã³rdenes se crearon hoy**
```kql
message: "Order created successfully" AND @timestamp >= now/d
```

### **4. Ver logs de configuraciÃ³n de sistema**
```kql
message: "System Configuration" OR message: "Database Configuration"
```

### **5. Ver logs de health checks**
```kql
message: "health" OR logger_name: "HealthCheck"
```

### **6. Ver logs de un thread especÃ­fico (debugging)**
```kql
thread_name: "reactor-http-nio-2"
```

### **7. Ver logs con stack traces (excepciones)**
```kql
_exists_: stack_trace
```

---

## ğŸ› ï¸ **PARTE 8: Troubleshooting**

### **Problema 1: No veo logs en Kibana**

#### **Checklist:**
1. âœ… Verificar que Elasticsearch estÃ¡ corriendo:
```bash
curl http://localhost:9200/_cluster/health
```

2. âœ… Verificar que Logstash estÃ¡ corriendo:
```bash
docker logs logstash | tail -20
```

3. âœ… Verificar que Spring Boot estÃ¡ enviando logs:
```bash
# Debe aparecer "Observability - Logstash logging enabled"
docker logs store-service | grep Logstash
```

4. âœ… Verificar que hay Ã­ndices en Elasticsearch:
```bash
curl http://localhost:9200/_cat/indices?v
```

5. âœ… Verificar que el Index Pattern en Kibana matchea:
```
Index pattern: app-logs-*
Ãndices existentes: app-logs-2025-12-18
```

---

### **Problema 2: Logs aparecen sin formato JSON**

**SoluciÃ³n:** Verificar `logback-spring.xml`:
```xml
<encoder class="net.logstash.logback.encoder.LogstashEncoder" />
```

---

### **Problema 3: Timezone incorrecto**

**SoluciÃ³n en Kibana:**
1. **Management** â†’ **Stack Management** â†’ **Advanced Settings**
2. Buscar: `dateFormat:tz`
3. Cambiar a: `America/Argentina/Buenos_Aires`

---

## ğŸ“Œ **PARTE 9: Comandos Ãštiles para Defensa**

### **Generar logs de prueba**
```bash
# Crear orden
curl -X POST http://localhost:8080/api/product-orders \
  -H "Content-Type: application/json" \
  -d '{"totalPrice": 1234.56, "paymentMethod": "CREDIT_CARD"}'

# Listar Ã³rdenes
curl http://localhost:8080/api/product-orders

# Ver health
curl http://localhost:8080/management/health
```

### **Ver logs directamente en Elasticsearch**
```bash
# Ãšltimos 5 logs
curl 'http://localhost:9200/app-logs-*/_search?size=5&sort=@timestamp:desc' | jq '.hits.hits[]._source'

# Logs de errores
curl 'http://localhost:9200/app-logs-*/_search?q=level:ERROR&size=10' | jq

# Contar logs por nivel
curl 'http://localhost:9200/app-logs-*/_search' \
  -H 'Content-Type: application/json' \
  -d '{"size":0,"aggs":{"levels":{"terms":{"field":"level.keyword"}}}}' | jq
```

### **Ver pipeline de Logstash en ejecuciÃ³n**
```bash
# Ver configuraciÃ³n cargada
docker exec logstash cat /usr/share/logstash/pipeline/logstash.conf

# Ver logs de Logstash
docker logs logstash --tail 50 --follow
```

---

## ğŸ“ **PARTE 10: Puntos Clave para la Defensa**

### **1. Flujo de Logs (explicar con diagrama)**
```
Spring Boot â†’ Logback (JSON) â†’ TCP:5044 â†’ Logstash â†’ Elasticsearch â†’ Kibana
```

### **2. Por quÃ© JSON en lugar de texto plano**
- âœ… Estructurado = bÃºsquedas mÃ¡s rÃ¡pidas
- âœ… Campos indexables (filtrar por `level`, `logger_name`, etc.)
- âœ… No necesita parseo con Grok (mÃ¡s rÃ¡pido)

### **3. Por quÃ© Ã­ndices diarios (`app-logs-%{+YYYY.MM.dd}`)**
- âœ… Performance: bÃºsquedas mÃ¡s rÃ¡pidas (menos datos por Ã­ndice)
- âœ… GestiÃ³n: fÃ¡cil eliminar logs antiguos (`DELETE /app-logs-2025-11-*`)
- âœ… Escalabilidad: distribuir Ã­ndices en mÃºltiples nodos

### **4. Ventajas de ELK vs Logs en archivo**
- âœ… Centralizado: todos los microservicios en un solo lugar
- âœ… BÃºsqueda: encontrar un log en 100GB en segundos
- âœ… Real-time: ver logs mientras suceden
- âœ… VisualizaciÃ³n: grÃ¡ficos, dashboards, alertas

### **5. Logs enriquecidos que agregamos**
- âœ… Startup: versiÃ³n Java, memoria, configuraciÃ³n BD
- âœ… Operaciones: crear/listar/eliminar Ã³rdenes con contexto
- âœ… Errores: mensajes descriptivos con stack traces
- âœ… Performance: contar items, total de registros

---

**Ãšltima actualizaciÃ³n: 18 Diciembre 2025**  
**Rama: feature/enrich-logging**
